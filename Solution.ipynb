{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retail Data Management and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective:\n",
    "\n",
    "- Participants will implement a fully modular ETL pipeline using Python. \n",
    "- They must:\n",
    "    - Use Python functions to manage Azure databricks database operations (setup, data loading, transformations).\n",
    "    - Analyze data using Python for Data Science.\n",
    "    - Use PySpark for advanced transformations.\n",
    "- Provide solution to the python functions according to the specifications.\n",
    "- __Refrain from modifying the boilerplate code as it may lead to unexpected behavior.__\n",
    "- The solution is to be written between the comments `# code starts here` and `# code ends here`.\n",
    "- On completing all the questions, the assessment is to be submitted on moodle for evaluation.\n",
    "- Before submitting the assessment make sure there are no errors while executing the assessment notebook.\n",
    "- The kernel of the Jupyter notebook is to be set as `Python 3.10.6` if not set already. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the libraries which are required to interact with Snowflake are imported here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from azure.identity import ClientSecretCredential\n",
    "from azure.mgmt.resource import ResourceManagementClient\n",
    "from azure.mgmt.databricks import AzureDatabricksManagementClient\n",
    "from pyspark.sql import SparkSession\n",
    "from databricks import sql\n",
    "import subprocess\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import pandas as pd\n",
    "from mongomock import MongoClient\n",
    "import mongomock\n",
    "from pyspark.sql import functions as F\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databricks details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_number = random.randint(1000, 9999)\n",
    "workspace_name = \"demoworkspace\"\n",
    "resource_group_name = \"Test\"\n",
    "location = \"eastus\"\n",
    "managed_resource_group_name = \"test-rg1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Azure Credentials\n",
    "\n",
    "Executes the process of loading Azure credentials from a JSON file and returning an authenticated credential object.\n",
    "\n",
    "This function reads the azure_credentials.json file (or a specified file path), extracts required authentication details (tenant_id, client_id, client_secret, and subscription_id), and validates their presence. It then authenticates using ClientSecretCredential, allowing secure access to Azure resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the file exists\n",
    "credentials_file= \"/home/labuser/Desktop/Project/azure_credentials.json\"\n",
    "if not os.path.exists(credentials_file):\n",
    "    raise FileNotFoundError(f\"The credentials file '{credentials_file}' does not exist.\")\n",
    "\n",
    "# Load credentials from the file\n",
    "with open(credentials_file, \"r\") as file:\n",
    "    creds = json.load(file)\n",
    "global tenant_id , client_id,client_secret,subscription_id,credential\n",
    "# Extract required fields\n",
    "tenant_id = creds.get(\"tenant_id\")\n",
    "client_id = creds.get(\"client_id\")\n",
    "client_secret = creds.get(\"client_secret\")\n",
    "subscription_id = creds.get(\"subscription_id\")\n",
    "\n",
    "# Validate required fields\n",
    "if not all([tenant_id, client_id, client_secret, subscription_id]):\n",
    "    raise ValueError(\"The credentials file is missing one or more required fields: 'tenant_id', 'client_id', 'client_secret', 'subscription_id'.\")\n",
    "\n",
    "# Authenticate using ClientSecretCredential\n",
    "credential = ClientSecretCredential(tenant_id=tenant_id, client_id=client_id, client_secret=client_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MongoDB Client Creation ; Loading Product and Order Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute the cell below to upload the data from the Products, Orders, Order_Details CSV file into product_collection, order_collection,order_details_collection onject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Mongo_Resources():\n",
    "    # Initialize mongomock client to simulate MongoDB\n",
    "    client = mongomock.MongoClient()\n",
    "\n",
    "    # Accessing the simulated MongoDB database\n",
    "    db = client['inventory_system']\n",
    "\n",
    "    order_collection = db['orders']\n",
    "    product_collection = db['products']\n",
    "    order_details_collection = db['order_details']\n",
    "\n",
    "    # Loading CSV files\n",
    "    orders_df = pd.read_csv('Orders.csv')\n",
    "    products_df = pd.read_csv('Products.csv')\n",
    "    order_details_df = pd.read_csv('Order_Details.csv')\n",
    "\n",
    "    # Convert DataFrames to dictionaries (for MongoDB insert)\n",
    "    orders_data = orders_df.to_dict(orient='records')\n",
    "    products_data = products_df.to_dict(orient='records')\n",
    "    order_details_data = order_details_df.to_dict(orient='records')\n",
    "\n",
    "    # Insert data into MongoDB collections\n",
    "    order_collection.insert_many(orders_data)\n",
    "    product_collection.insert_many(products_data)\n",
    "    order_details_collection.insert_many(order_details_data)\n",
    "\n",
    "    # Verify insertion by printing counts of documents in each collection\n",
    "    print(f\"Orders count: {order_collection.count_documents({})}\")\n",
    "    print(f\"Products count: {product_collection.count_documents({})}\")\n",
    "    print(f\"Order Details count: {order_details_collection.count_documents({})}\")\n",
    "\n",
    "    return product_collection,order_collection,order_details_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orders count: 50\n",
      "Products count: 20\n",
      "Order Details count: 100\n"
     ]
    }
   ],
   "source": [
    "products_collection, orders_collection,order_details_collection = create_Mongo_Resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Extract data from mongodb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The function `extract_data_from_mongodb` extracts the data from the given MongoDB collections and returns pandas Dataframes.\n",
    "* Retrieve all documents from the products_collection in MongoDB, including only the __\"ProductId\", \"Name\", \"Category\", \"Price\" and \"Stock\"__ fields for each document. The result in the form of list must be converted into Pandas dataframe.\n",
    "* Retrieve all documents from the orders_collection in MongoDB, including only the __\"OrderId\", \"CustomerId\", and \"Status\"__ fields for each document. The result in the form of list must be converted into Pandas dataframe.\n",
    "* Retrieve all documents from the order_details_collection in MongoDB, including only the __\"OrderDetailId\", \"OrderId\", \"ProductId\" and \"Quantity\"__ fields for each document. The result in the form of list must be converted into Pandas dataframe.\n",
    "* Arguments: \n",
    "    - products_collection: Products collection in MongoDB\n",
    "    - orders_collection: Orders collection in MongoDB\n",
    "    - order_details_collection : Order Details collection in the MongoDB\n",
    "* Returns:\n",
    "    - products_df: Pandas dataframe representing data in Products collection with the specified columns\n",
    "    - orders_df: Pandas dataframe representing data in Orders collection with the specified columns\n",
    "    - order_details_df: Pandas dataframe representing data in Order Details collection with the specified columns\n",
    "* Sample Output:\n",
    "```\n",
    "Products Data:\n",
    "   ProductId          Name     Category  Price  Stock\n",
    "0          1      iPhone15  Electronics    999    100\n",
    "1          2     GalaxyS24  Electronics    899     80\n",
    "\n",
    "Orders Data:\n",
    "   OrderId  CustomerId     Status\n",
    "0        1         125   Canceled\n",
    "1        2         187  Delivered\n",
    "\n",
    "Order Details Data:\n",
    "   OrderDetailId  OrderId  ProductId  Quantity\n",
    "0              1        9          1         3\n",
    "1              2        2          8         2\n",
    "```\n",
    "* Your solution should be provided within __code starts here__ and __code ends here__ block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_mongodb(products_collection, orders_collection, order_details_collection): \n",
    "    orders_df, products_df,order_details_df = None, None, None\n",
    "    # code starts here\n",
    "    # Extract Products data\n",
    "    products = list(products_collection.find({}, {\"_id\": 0, \"ProductId\": 1, \"Name\":1,\"Category\":1,\"Price\": 1,\"Stock\": 1}))\n",
    "    products_df = pd.DataFrame(products)\n",
    "\n",
    "    # Extract Orders data\n",
    "    orders = list(orders_collection.find({}, {\"_id\": 0, \"OrderId\": 1, \"CustomerId\":1,\"Status\":1}))\n",
    "    orders_df = pd.DataFrame(orders)   \n",
    "\n",
    "    # Extract Order Details data\n",
    "    order_details = list(order_details_collection.find({}, {\"_id\": 0, \"OrderDetailId\": 1, \"OrderId\":1,\"ProductId\":1,\"Quantity\": 1}))\n",
    "    order_details_df = pd.DataFrame(order_details) \n",
    "    # code ends here\n",
    "    return products_df, orders_df,order_details_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Products Data:\n",
      "   ProductId          Name     Category  Price  Stock\n",
      "0          1      iPhone15  Electronics    999    100\n",
      "1          2     GalaxyS24  Electronics    899     80\n",
      "2          3    AirPodsPro  Electronics    249    150\n",
      "3          4    MacBookPro  Electronics   1999     50\n",
      "4          5  PlayStation5  Electronics    499     70\n",
      "Orders Data:\n",
      "   OrderId  CustomerId     Status\n",
      "0        1         125   Canceled\n",
      "1        2         187  Delivered\n",
      "2        3         186    Pending\n",
      "3        4         137   Canceled\n",
      "4        5         194    Shipped\n",
      "Order Details Data:\n",
      "   OrderDetailId  OrderId  ProductId  Quantity\n",
      "0              1        9          1         3\n",
      "1              2        2          8         2\n",
      "2              3       22         11         1\n",
      "3              4       33         14         2\n",
      "4              5       24         10         2\n"
     ]
    }
   ],
   "source": [
    "products_df, orders_df,order_details_df = extract_data_from_mongodb(products_collection, orders_collection, order_details_collection)\n",
    "# if (products_df != None and orders_df != None and order_details_df != None):\n",
    "# print(\"Data extraction failed.\")\n",
    "print(\"\\nProducts Data:\")\n",
    "print(products_df.head())\n",
    "print(\"Orders Data:\")\n",
    "print(orders_df.head())\n",
    "print(\"Order Details Data:\")\n",
    "print(order_details_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating databricks workspace and retrieving token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_databricks_workspace_and_get_token(\n",
    "    credential,\n",
    "    subscription_id,\n",
    "    resource_group_name,\n",
    "    managed_resource_group_name,\n",
    "    workspace_name,\n",
    "    location\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a Databricks workspace and returns the workspace URL and access token.\n",
    "\n",
    "    Args:\n",
    "        credential (DefaultAzureCredential): Azure DefaultAzureCredential.\n",
    "        subscription_id (str): Azure subscription ID.\n",
    "        resource_group_name (str): Name of the Azure resource group.\n",
    "        managed_resource_group_name (str): Managed resource group name for Databricks.\n",
    "        workspace_name (str): Name of the Databricks workspace.\n",
    "        location (str): Azure region for the workspace.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the workspace URL and access token.\n",
    "    \"\"\"\n",
    "    # Initialize Resource Management Client\n",
    "    resource_client = ResourceManagementClient(credential, subscription_id)\n",
    "\n",
    "    # Create the resource group if it does not exist\n",
    "    print(f\"Ensuring resource group '{resource_group_name}' exists in '{location}'...\")\n",
    "    resource_client.resource_groups.create_or_update(\n",
    "        resource_group_name,\n",
    "        {\"location\": location}\n",
    "    )\n",
    "    print(f\"Resource group '{resource_group_name}' is ready.\")\n",
    "\n",
    "    # Initialize Databricks Management Client\n",
    "    databricks_client = AzureDatabricksManagementClient(credential, subscription_id)\n",
    "\n",
    "    # Construct managed resource group ID\n",
    "    managed_resource_group_id = f\"/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}-managed\"\n",
    "\n",
    "    # Create the Databricks workspace\n",
    "    print(f\"Creating Databricks workspace '{workspace_name}' in '{location}'...\")\n",
    "    workspace = databricks_client.workspaces.begin_create_or_update(\n",
    "        resource_group_name=resource_group_name,\n",
    "        workspace_name=workspace_name,\n",
    "        parameters={\n",
    "            \"location\": location,\n",
    "            \"managed_resource_group_id\": managed_resource_group_id,\n",
    "            \"sku\": {\"name\": \"premium\"}\n",
    "        }\n",
    "    ).result()\n",
    "\n",
    "    print(f\"Databricks workspace '{workspace_name}' created successfully.\")\n",
    "    # print(workspace.workspace_url)\n",
    "    # workspaceurl = workspace.workspace_url\n",
    "    # Retrieve the workspace URL\n",
    "    \n",
    " \n",
    "    return workspace_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensuring resource group 'Test1' exists in 'eastus'...\n",
      "Resource group 'Test1' is ready.\n",
      "Creating Databricks workspace 'demoworkspace2379' in 'eastus'...\n",
      "Databricks workspace 'demoworkspace2379' created successfully.\n"
     ]
    }
   ],
   "source": [
    "workspace_name = create_databricks_workspace_and_get_token(\n",
    "        credential,\n",
    "        subscription_id,\n",
    "        resource_group_name,\n",
    "        managed_resource_group_name,\n",
    "        workspace_name,\n",
    "        location\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Databricks HTTP Path\n",
    "\n",
    "Returns the Databricks server hostname and the HTTP path for database connections which will be using for connection creation for the databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_databricks_http_path(credentials_file=\"azure_credentials.json\"):\n",
    "    \"\"\"\n",
    "    Retrieves the Databricks HTTP Path for connection.\n",
    "\n",
    "    Args:\n",
    "        credentials_file (str): Path to Azure credentials file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (Databricks Server Hostname, HTTP Path)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get Databricks workspace URL\n",
    "    workspace_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.Databricks/workspaces/{workspace_name}?api-version=2018-04-01\"\n",
    "\n",
    "    access_token = credential.get_token(\"https://management.azure.com/.default\").token\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(workspace_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to retrieve Databricks workspace: {response.text}\")\n",
    "\n",
    "    workspace_data = response.json()\n",
    "    databricks_host = workspace_data[\"properties\"][\"workspaceUrl\"]\n",
    "\n",
    "    # Get SQL Warehouses from Databricks\n",
    "    sql_endpoint_url = f\"https://{databricks_host}/api/2.0/sql/warehouses\"\n",
    "    db_access_token = credential.get_token(\"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default\").token  # Databricks token\n",
    "\n",
    "    db_headers = {\n",
    "        \"Authorization\": f\"Bearer {db_access_token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    sql_response = requests.get(sql_endpoint_url, headers=db_headers)\n",
    "    if sql_response.status_code != 200:\n",
    "        raise Exception(f\"Failed to retrieve SQL Warehouses: {sql_response.text}\")\n",
    "\n",
    "    warehouses = sql_response.json()[\"warehouses\"]\n",
    "    if not warehouses:\n",
    "        raise Exception(\"No SQL Warehouses found in Databricks.\")\n",
    "\n",
    "    warehouse = warehouses[0]  # Selecting the first available warehouse\n",
    "    http_path = warehouse[\"odbc_params\"][\"path\"]\n",
    "\n",
    "    return databricks_host, http_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adb-2564247772168554.14.azuredatabricks.net\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/sql/1.0/warehouses/e311dbfc0840100b'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "databricks_url, databricks_http_path = get_databricks_http_path()\n",
    "print(databricks_url)\n",
    "databricks_http_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create token of the databricks using the below steps\n",
    "\n",
    "Login to databricks with the credentails , and Generate a personal access token and provide here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token():\n",
    "    token = \"dapi9ad66483a1783de585529a283bd34884-3\"\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "global workspaceurl,access_token\n",
    "access_token = get_token()\n",
    "\n",
    "workspaceurl = \"https://\"+databricks_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Spark Dataframe creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The function `create_spark_dataframe` initializes a Spark session and converts three given Pandas DataFrames (products_df, orders_df, order_details_df) into corresponding Spark DataFrames (products_spark_df, orders_spark_df, orderdetails_spark_df).\n",
    "* Create a Spark session with the application name \"InventoryProcessing\".\n",
    "* Convert the product, orders and order details dataframe to __Spark dataframe__\n",
    "* Arguments: \n",
    "    - products_df: Pandas Dataframe representing Products \n",
    "    - orders_df: Pandas Dataframe representing Orders \n",
    "    - order_details_df : Pandas Dataframe representing Order Details \n",
    "* Returns:\n",
    "    - products_spark_df: Spark DataFrame created from the products Pandas DataFrame.\n",
    "    - orders_spark_df: Spark DataFrame created from the orders Pandas DataFrame.\n",
    "    - orderdetails_spark_df: Spark DataFrame created from the order details Pandas DataFrame.\n",
    "* Your solution should be provided within __code starts here__ and __code ends here__ block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_dataframe(products_df, orders_df, order_details_df):\n",
    "    products_spark_df, orders_spark_df, orderdetails_spark_df = None,None,None\n",
    "    # code starts here\n",
    "    # Initialize Spark Session\n",
    "    spark = SparkSession.builder.appName(\"InventoryProcessing\").getOrCreate()\n",
    "\n",
    "    # Convert orders and products dataframes to Spark DataFrames\n",
    "    products_spark_df = spark.createDataFrame(products_df)\n",
    "    orders_spark_df = spark.createDataFrame(orders_df)\n",
    "    orderdetails_spark_df = spark.createDataFrame(order_details_df) \n",
    "    \n",
    "    # code ends here\n",
    "    return products_spark_df, orders_spark_df, orderdetails_spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/12 17:55:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "products_spark_df, orders_spark_df, orderdetails_spark_df = create_spark_dataframe(products_df, orders_df, order_details_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Using Spark, analyse the orders dataframe and find out the top 5 customers who placed the most orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The function `get_top5_customers_by_orders` analyzes the orders_spark_df Spark DataFrame to identify the top 5 customers who placed the highest number of orders. It returns a Spark DataFrame containing the CustomerId and OrderCount for these 5 top customers, sorted in descending order of OrderCount.\n",
    "* Arguments: \n",
    "    - orders_spark_df: Spark DataFrame containing order details.\n",
    "* Returns:\n",
    "    - top_customers_df (pyspark.sql.DataFrame): Spark DataFrame containing CustomerId and OrderCount of the top 5 customers.\n",
    "* Make sure the column names are as specified\n",
    "* Sample return data\n",
    "```\n",
    "        +----------+----------+\n",
    "        |CustomerId|OrderCount|\n",
    "        +----------+----------+\n",
    "        |       146|         4|\n",
    "        |       110|         3|\n",
    "```\n",
    "* Your solution should be provided within __code starts here__ and __code ends here__ block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top5_customers_by_orders(orders_spark_df):\n",
    "    top_customers_df = None  \n",
    "    # code starts here\n",
    "    try:\n",
    "        # Group by CustomerId and count the number of orders\n",
    "        customer_order_count = orders_spark_df.groupBy(\"CustomerId\").agg(\n",
    "            F.count(\"OrderId\").alias(\"OrderCount\")\n",
    "        )\n",
    "        \n",
    "        # Order by the count in descending order and get the top 5\n",
    "        top_customers_df = customer_order_count.orderBy(F.desc(\"OrderCount\")).limit(5)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while calculating top customers: {e}\")\n",
    "    # code ends here\n",
    "    return top_customers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|CustomerId|OrderCount|\n",
      "+----------+----------+\n",
      "|       146|         4|\n",
      "|       110|         3|\n",
      "|       170|         2|\n",
      "|       137|         2|\n",
      "|       165|         2|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top5_customers_df = get_top5_customers_by_orders(orders_spark_df)\n",
    "if (top5_customers_df!= None):\n",
    "    top5_customers_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Calculate Top 5 Revenue Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The function `calculate_top5_revenue_products` analyzes the orderdetails_spark_df and products_spark_df Spark DataFrames to calculate the top 5 products that generated the highest total revenue. It returns a Spark DataFrame containing the ProductId, ProductName, and TotalRevenue for these top 5 products, sorted in descending order of TotalRevenue.\n",
    "* Rename `ProductId` column in products DataFrame to `ProductId_Products` avoid ambiguity\n",
    "* Rename `Name` column in joined dataframe to `ProductName`\n",
    "* Calculate `TotalRevenue` as Quantity * Price\n",
    "* Arguments:\n",
    "    - products_spark_df (pyspark.sql.DataFrame): Spark DataFrame containing product details, including ProductId, Name (ProductName), and Price.\n",
    "    - orderdetails_spark_df (pyspark.sql.DataFrame): Spark DataFrame containing order details, including ProductId, Quantity, and other order-specific details.    \n",
    "* Returns:\n",
    "    - top_revenue_products_df (pyspark.sql.DataFrame): Spark DataFrame containing __ProductId, ProductName, and TotalRevenue__ of the top 5 revenue-generating products.\n",
    "* Make sure the column names are as specified\n",
    "* Sample return data:\n",
    "```\n",
    "        +------------------+--------------+------------+\n",
    "        |ProductId_Products|   ProductName|TotalRevenue|\n",
    "        +------------------+--------------+------------+\n",
    "        |                 1|      iPhone15|       30969|\n",
    "        |                11|  Refrigerator|       22485|\n",
    "```\n",
    "* Your solution should be provided within __code starts here__ and __code ends here__ block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_top5_revenue_products(products_spark_df, orderdetails_spark_df):\n",
    "    top5_revenue_products = None\n",
    "    # code starts here\n",
    "    # Rename ProductId column in products DataFrame to avoid ambiguity\n",
    "    products_spark_df = products_spark_df.withColumnRenamed(\"ProductId\", \"ProductId_Products\")\n",
    "\n",
    "    # Join order_details with products DataFrame to get product prices\n",
    "    joined_df = orderdetails_spark_df.join(\n",
    "        products_spark_df,\n",
    "        orderdetails_spark_df.ProductId == products_spark_df.ProductId_Products\n",
    "    )\n",
    "\n",
    "    # Rename columns to avoid ambiguity\n",
    "    joined_df = joined_df.withColumnRenamed(\"ProductId\", \"ProductId_Orders\")\n",
    "    joined_df = joined_df.withColumnRenamed(\"Name\", \"ProductName\")\n",
    "\n",
    "    # Calculate total revenue for each product\n",
    "    revenue_df = joined_df.withColumn(\"Revenue\", joined_df.Quantity * joined_df.Price)\n",
    "\n",
    "    # Group by ProductId and ProductName, and calculate total revenue\n",
    "    revenue_per_product = revenue_df.groupBy(\"ProductId_Products\", \"ProductName\").agg(\n",
    "        F.sum(\"Revenue\").alias(\"TotalRevenue\")\n",
    "    )\n",
    "\n",
    "    # Order by TotalRevenue in descending order and return the top 5 products\n",
    "    top5_revenue_products = revenue_per_product.orderBy(F.desc(\"TotalRevenue\")).limit(5) \n",
    "    \n",
    "    # code ends here\n",
    "    return top5_revenue_products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+------------+\n",
      "|ProductId_Products|   ProductName|TotalRevenue|\n",
      "+------------------+--------------+------------+\n",
      "|                 1|      iPhone15|       30969|\n",
      "|                11|  Refrigerator|       22485|\n",
      "|                 5|  PlayStation5|       12475|\n",
      "|                 6|   XboxSeriesX|       12475|\n",
      "|                12|WashingMachine|        9588|\n",
      "+------------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top5_revenue_products = calculate_top5_revenue_products(products_spark_df,orderdetails_spark_df)\n",
    "if top5_revenue_products!= None:\n",
    "    top5_revenue_products.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Transform data to get detailed order details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The function `transform_order_details` transforms the order details data in orderdetails_spark_df by joining it with orders_spark_df and products_spark_df to include customer and product details, and calculates the total value for each order detail.\n",
    "* The final_order_details_spark_df should have only the following columns:\n",
    "    - OrderDetailId,\n",
    "    - OrderId,\n",
    "    - ProductId,\n",
    "    - CustomerId,\n",
    "    - Name,\n",
    "    - Category,\n",
    "    - Quantity,\n",
    "    - Price,\n",
    "    - TotalValue\n",
    "* Args:\n",
    "    - products_spark_df (pyspark.sql.DataFrame): Spark DataFrame containing product details.\n",
    "    - orders_spark_df (pyspark.sql.DataFrame): Spark DataFrame containing order information.\n",
    "    - orderdetails_spark_df (pyspark.sql.DataFrame): Spark DataFrame containing order details.\n",
    "* Returns:\n",
    "    - final_order_details_spark_df (pyspark.sql.DataFrame): Transformed Spark DataFrame containing enhanced order details \n",
    "        with selected columns and calculated TotalValue.\n",
    "* Make sure the column names are as specified\n",
    "* Sample return data:\n",
    "```\n",
    "        +-------------+-------+---------+----------+------------+-----------+--------+-----+----------+\n",
    "        |OrderDetailId|OrderId|ProductId|CustomerId|        Name|   Category|Quantity|Price|TotalValue|\n",
    "        +-------------+-------+---------+----------+------------+-----------+--------+-----+----------+\n",
    "        |           90|      3|        5|       186|PlayStation5|Electronics|       4|  499|      1996|\n",
    "        |           98|     32|        5|       164|PlayStation5|Electronics|       2|  499|       998|\n",
    "        |           91|     34|        5|       165|PlayStation5|Electronics|       5|  499|      2495|\n",
    "```\n",
    "* Your solution should be provided within __code starts here__ and __code ends here__ block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_order_details(products_spark_df, orders_spark_df, orderdetails_spark_df):\n",
    "    final_order_details_spark_df = None\n",
    "    # Code starts here\n",
    "    # Step 1: Renaming columns in orders DataFrame to avoid conflicts\n",
    "    orders_spark_df = orders_spark_df.withColumnRenamed(\"OrderId\", \"OrderId_Orders\")\n",
    "\n",
    "    # Step 2: Join order_details with orders DataFrame to get CustomerId\n",
    "    order_with_customer_df = orderdetails_spark_df.join(\n",
    "        orders_spark_df,\n",
    "        orderdetails_spark_df.OrderId == orders_spark_df.OrderId_Orders,\n",
    "        \"inner\"  # Use \"inner\" join\n",
    "    )\n",
    "\n",
    "    # Step 3: Rename ProductId column in products DataFrame to avoid conflicts\n",
    "    products_spark_df = products_spark_df.withColumnRenamed(\"ProductId\", \"ProductId_Products\")\n",
    "\n",
    "    # Step 4: Join the resulting DataFrame with products DataFrame to get product details (ProductName, Category)\n",
    "    enhanced_order_details_df = order_with_customer_df.join(\n",
    "        products_spark_df,\n",
    "        order_with_customer_df.ProductId == products_spark_df.ProductId_Products,\n",
    "        \"inner\"  # Use \"inner\" join\n",
    "    )\n",
    "\n",
    "    # Step 5: Calculate TotalValue (Quantity * Price)\n",
    "    enhanced_order_details_df = enhanced_order_details_df.withColumn(\n",
    "        \"TotalValue\",\n",
    "        enhanced_order_details_df.Quantity * enhanced_order_details_df.Price\n",
    "    )\n",
    "    # Step 6: Select relevant columns for enhanced report\n",
    "    final_order_details_spark_df = enhanced_order_details_df.select(\n",
    "        \"OrderDetailId\",\n",
    "        \"OrderId\",\n",
    "        \"ProductId\",\n",
    "        \"CustomerId\",\n",
    "        \"Name\",\n",
    "        \"Category\",\n",
    "        \"Quantity\",\n",
    "        \"Price\",\n",
    "        \"TotalValue\"\n",
    "    ) \n",
    "    \n",
    "    # Code ends here\n",
    "    return final_order_details_spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+---------+----------+------------+-----------+--------+-----+----------+\n",
      "|OrderDetailId|OrderId|ProductId|CustomerId|        Name|   Category|Quantity|Price|TotalValue|\n",
      "+-------------+-------+---------+----------+------------+-----------+--------+-----+----------+\n",
      "|           97|     16|        7|       170|  SmartWatch|  Wearables|       1|  199|       199|\n",
      "|           83|     16|        7|       170|  SmartWatch|  Wearables|       5|  199|       995|\n",
      "|           66|     22|        7|       168|  SmartWatch|  Wearables|       3|  199|       597|\n",
      "|           15|     11|        7|       137|  SmartWatch|  Wearables|       2|  199|       398|\n",
      "|           77|     47|        6|       118| XboxSeriesX|Electronics|       5|  499|      2495|\n",
      "|           56|     47|        6|       118| XboxSeriesX|Electronics|       1|  499|       499|\n",
      "|           94|     30|        6|       110| XboxSeriesX|Electronics|       3|  499|      1497|\n",
      "|           63|      2|        6|       187| XboxSeriesX|Electronics|       4|  499|      1996|\n",
      "|           95|     49|        6|       189| XboxSeriesX|Electronics|       5|  499|      2495|\n",
      "|           86|     39|        6|       147| XboxSeriesX|Electronics|       1|  499|       499|\n",
      "|           40|     35|        6|       143| XboxSeriesX|Electronics|       4|  499|      1996|\n",
      "|           37|     37|        6|       183| XboxSeriesX|Electronics|       2|  499|       998|\n",
      "|           72|     47|        9|       118|     Blender|    Kitchen|       3|   99|       297|\n",
      "|           31|     47|        9|       118|     Blender|    Kitchen|       2|   99|       198|\n",
      "|           13|     46|        9|       110|     Blender|    Kitchen|       1|   99|        99|\n",
      "|           50|     49|        9|       189|     Blender|    Kitchen|       1|   99|        99|\n",
      "|           14|     49|        9|       189|     Blender|    Kitchen|       2|   99|       198|\n",
      "|           42|     10|        9|       107|     Blender|    Kitchen|       3|   99|       297|\n",
      "|           67|     49|        5|       189|PlayStation5|Electronics|       3|  499|      1497|\n",
      "|           90|      3|        5|       186|PlayStation5|Electronics|       4|  499|      1996|\n",
      "+-------------+-------+---------+----------+------------+-----------+--------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_order_details_spark_df = transform_order_details(products_spark_df, orders_spark_df, orderdetails_spark_df)\n",
    "if final_order_details_spark_df != None:\n",
    "    final_order_details_spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing a Databricks SQL Connection\n",
    "\n",
    "The function get_databricks_connection establishes and returns a connection to a Databricks SQL warehouse or cluster using the Databricks SQL Connector for Python. It is essential to close the connection after use to free up resources.\n",
    "\n",
    "Process:\n",
    "\n",
    "Set up connection details using the Databricks workspace URL, HTTP path, and access token. Establish a connection using the sql.connect method from the Databricks SQL Connector for Python. Args:\n",
    "\n",
    "None Returns:\n",
    "\n",
    "conn (sql.Connection): The Databricks SQL connection object.\n",
    "\n",
    "Error Handling:\n",
    "\n",
    "The function checks if all necessary environment variables (DATABRICKS_SERVER_HOSTNAME, DATABRICKS_HTTP_PATH, DATABRICKS_ACCESS_TOKEN) are set. If any are missing, it raises a ValueError. If the connection attempt fails, it catches the exception, prints an error message, and returns None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_databricks_connection():\n",
    "    \"\"\"\n",
    "    Establish and return a Databricks SQL connection.\n",
    "    Ensure to close the connection after use.\n",
    "\n",
    "    Returns:\n",
    "        conn (sql.Connection): Databricks SQL connection object.\n",
    "    \"\"\"\n",
    "    # Set up connection details\n",
    "    DATABRICKS_SERVER_HOSTNAME = workspaceurl  # Replace with your Databricks workspace URL\n",
    "    DATABRICKS_HTTP_PATH = databricks_http_path # Replace with your warehouse's HTTP Path\n",
    "    DATABRICKS_ACCESS_TOKEN = access_token  # Replace with your access token\n",
    "\n",
    "    # Establish connection\n",
    "    conn = sql.connect(\n",
    "        server_hostname=DATABRICKS_SERVER_HOSTNAME,\n",
    "        http_path=DATABRICKS_HTTP_PATH,\n",
    "        access_token=DATABRICKS_ACCESS_TOKEN\n",
    "    )\n",
    "\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<databricks.sql.client.Connection at 0x7e1498dd2bf0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global conn\n",
    "conn = get_databricks_connection()\n",
    "conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: Create snowflake Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This function `setup_database_and_table` creates a database named __PRODUCT_INVENTORY__ and a schema named __ANALYTICS__ in the Snowflake instance if they do not already exist, using the provided cursor object.\n",
    "* Create table named __TABLE ORDER_DETAILS__ in __PRODUCT_INVENTORY__ and schema __ANALYTICS__\n",
    "* Table __TABLE ORDER_DETAILS__ should have the following schema:\n",
    "    - OrderDetailId (INT): Unique identifier for each order detail record.\n",
    "    - OrderId (INT): Identifier for the associated order.\n",
    "    - ProductId (INT): Identifier for the product.\n",
    "    - CustomerId (INT): Identifier for the customer who placed the order.\n",
    "    - Name (STRING): Name of the product.\n",
    "    - Category (STRING): Category of the product.\n",
    "    - Quantity (INT): Number of units ordered.\n",
    "    - Price (FLOAT): Price per unit of the product.\n",
    "    - TotalValue (FLOAT): Total value of the order detail (calculated as `Quantity * Price`).\n",
    "* The database, schema, and table are created using SQL commands executed through the provided Snowflake cursor.\n",
    "* Arguments:\n",
    "    - cursor (snowflake.connector.cursor.SnowflakeCursor): A Snowflake cursor object used to execute SQL commands.\n",
    "* Returns:\n",
    "    - result (str): A message indicating the result of the database creation operation.\n",
    "        - Success: \"Database, schema, and table setup successfully.\"\n",
    "        - Failure: \"An error occurred during setup: {e}\"\n",
    "* Your solution should be provided within __code starts here__ and __code ends here__ block.\n",
    "* Implement suitable error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_database_and_table(conn):\n",
    "    # Code starts here\n",
    "\n",
    "    # Step 2: Create table if it does not exist\n",
    "    create_query = \"\"\"\n",
    "    CREATE OR REPLACE TABLE ORDER_DETAILS (\n",
    "        OrderDetailId INT,\n",
    "        OrderId INT,\n",
    "        ProductId INT,\n",
    "        CustomerId INT,\n",
    "        Name STRING,\n",
    "        Category STRING,\n",
    "        Quantity INT,\n",
    "        Price FLOAT,\n",
    "        TotalValue FLOAT\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    with conn.cursor() as cursor:\n",
    "        \n",
    "        result = cursor.execute(create_query)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<databricks.sql.client.Cursor at 0x7e1498dd31c0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = setup_database_and_table(conn)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8: Insert data into the ORDER_DETAILS table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This function `insert_order_details_into_snowflake` inserts order details into the Snowflake ORDER_DETAILS table using the provided Snowflake cursor object.\n",
    "* Arguments:\n",
    "    - cursor (snowflake.connector.cursor.SnowflakeCursor): A Snowflake cursor object used to execute SQL commands.\n",
    "    - final_order_details_df (pandas.DataFrame): A DataFrame containing the final order details to be inserted into the Snowflake ORDER_DETAILS table.\n",
    "* Returns:\n",
    "    - insert_status (str): A message indicating the result of the insertion operation.\n",
    "        - Success: \"Order details data successfully inserted into ORDER_DETAILS.\"\n",
    "        - Failure: \"Error occurred while inserting order details data: {e}\"\n",
    "* Your solution should be provided within code starts here and code ends here block.\n",
    "* Suitable error handling is implemented to capture and return any issues during the insertion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_order_details_into_sql(conn, final_order_details_spark_df):\n",
    "    insert_status = None\n",
    "    # Code starts here\n",
    "    try:\n",
    "        # Convert PySpark DataFrame to Pandas DataFrame if necessary\n",
    "        if hasattr(final_order_details_spark_df, \"toPandas\"):\n",
    "            final_order_details_df = final_order_details_spark_df.toPandas()\n",
    "\n",
    "        # Ensure data types are appropriate for the columns\n",
    "        final_order_details_df['OrderDetailId'] = final_order_details_df['OrderDetailId'].astype(int)\n",
    "        final_order_details_df['OrderId'] = final_order_details_df['OrderId'].astype(int)\n",
    "        final_order_details_df['ProductId'] = final_order_details_df['ProductId'].astype(int)\n",
    "        final_order_details_df['CustomerId'] = final_order_details_df['CustomerId'].astype(int)\n",
    "        final_order_details_df['Name'] = final_order_details_df['Name'].astype(str)\n",
    "        final_order_details_df['Category'] = final_order_details_df['Category'].astype(str)\n",
    "        final_order_details_df['Quantity'] = final_order_details_df['Quantity'].astype(int)\n",
    "        final_order_details_df['Price'] = final_order_details_df['Price'].astype(float)\n",
    "        final_order_details_df['TotalValue'] = final_order_details_df['TotalValue'].apply(lambda x: float(x))\n",
    "\n",
    "        # Iterate over the rows in the Pandas DataFrame\n",
    "        for index, row in final_order_details_df.iterrows():\n",
    "            # Explicitly cast to Python's native types\n",
    "            order_detail_id = int(row['OrderDetailId'])\n",
    "            order_id = int(row['OrderId'])\n",
    "            product_id = int(row['ProductId'])\n",
    "            customer_id = int(row['CustomerId'])\n",
    "            product_name = str(row['Name'])\n",
    "            category = str(row['Category'])\n",
    "            quantity = int(row['Quantity'])\n",
    "            price = float(row['Price'])\n",
    "            total_value = float(row['TotalValue'])\n",
    "\n",
    "            # Construct the SQL insert statement\n",
    "            insert_sql = \"\"\"\n",
    "            INSERT INTO ORDER_DETAILS (OrderDetailId, OrderId, ProductId, CustomerId, Name, Category, Quantity, Price, TotalValue)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\"\n",
    "            # Execute the SQL statement with the values\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(insert_sql, (order_detail_id, order_id, product_id, customer_id,\n",
    "                                                product_name, category, quantity, price, total_value))\n",
    " \n",
    "        insert_status = \"Order details data successfully inserted into ORDER_DETAILS.\"\n",
    "    except Exception as e:\n",
    "        # Catch any unforeseen errors during the iteration over the Pandas DataFrame\n",
    "        insert_status = f\"Error occurred while inserting order details data: : {str(e)}\" \n",
    "    # Code ends here\n",
    "    return insert_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Order details data successfully inserted into ORDER_DETAILS.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insert_status = insert_order_details_into_sql(conn, final_order_details_spark_df)\n",
    "insert_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9: Find the total revenue by category from the ORDER_DETAILS table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This function `find_total_revenue_by_category` fetches the total revenue for each product category from the ORDER_DETAILS table.\n",
    "* The result should be ordered by Total Revenue in Descending Order.\n",
    "* Covert the result from the SQL Query into a dictionary \n",
    "* Arguments: \n",
    "    - cursor: Snowflake database cursor.\n",
    "* Returns:\n",
    "    - category_revenue (list): A list of dictionaries containing the Category and TotalRevenue of each category.\n",
    "* Sample return data:\n",
    "```\n",
    "        [{'Category': 'Electronics', 'TotalRevenue': 70347.0},\n",
    "        {'Category': 'Appliances', 'TotalRevenue': 34469.0}....]\n",
    "```\n",
    "* Your solution should be provided within code starts here and code ends here block.\n",
    "* Suitable error handling is implemented to capture and return any issues during the insertion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_total_revenue_by_category(conn):\n",
    "    category_revenue  = []\n",
    "    # Code starts here\n",
    "    try:\n",
    "        with conn.cursor() as cursor:  # Cursor opens here\n",
    "            # Use the appropriate database and schema\n",
    "            # cursor.execute(\"USE DATABASE PRODUCT_INVENTORY;\")\n",
    "            # cursor.execute(\"USE SCHEMA ANALYTICS;\")\n",
    "\n",
    "            # SQL query to find total revenue by category\n",
    "            query = \"\"\"\n",
    "                SELECT Category, SUM(TotalValue) AS TotalRevenue\n",
    "                FROM ORDER_DETAILS\n",
    "                GROUP BY Category\n",
    "                ORDER BY TotalRevenue DESC;\n",
    "            \"\"\"\n",
    "            cursor.execute(query)\n",
    "            results = cursor.fetchall()\n",
    "\n",
    "            # Convert results to a list of dictionaries\n",
    "            for row in results:\n",
    "                category_revenue.append({\n",
    "                    \"Category\": row[0],\n",
    "                    \"TotalRevenue\": row[1]\n",
    "                })\n",
    "    except Exception as e:\n",
    "        # Handle errors and return an error message in the result\n",
    "        category_revenue = [{\"Error\": f\"Error occurred: {str(e)}\"}]\n",
    "    # code ends here\n",
    "    return category_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Category': 'Electronics', 'TotalRevenue': 70347.0},\n",
       " {'Category': 'Appliances', 'TotalRevenue': 34469.0},\n",
       " {'Category': 'Home', 'TotalRevenue': 8767.0},\n",
       " {'Category': 'Transportation', 'TotalRevenue': 6986.0},\n",
       " {'Category': 'Sports', 'TotalRevenue': 6746.0},\n",
       " {'Category': 'Kitchen', 'TotalRevenue': 6338.0},\n",
       " {'Category': 'Wearables', 'TotalRevenue': 2189.0}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_revenue = find_total_revenue_by_category(conn)\n",
    "category_revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------- END OF ASSESSMENT --------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
